Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 24
Rules claiming more threads will be scaled down.
Job stats:
job                 count
----------------  -------
tex2pdf_with_bib        1
total                   1

Resources before job selection: {'_cores': 24, '_nodes': 9223372036854775807}
Ready jobs (1)
Select jobs to execute...
Using greedy selector because only single job has to be scheduled.
Selected jobs (1)
Resources after job selection: {'_cores': 23, '_nodes': 9223372036854775806}

[Thu Nov 14 13:06:59 2024]
rule tex2pdf_with_bib:
    input: workflow/hcg_paper.tex
    output: outputs/publication/hcg_paper.pdf
    jobid: 0
    reason: Updated input files: workflow/hcg_paper.tex
    resources: mem_mb=1000, mem_mib=954, disk_mb=1000, disk_mib=954, tmpdir=/data

[Thu Nov 14 13:07:12 2024]
Finished job 0.
1 of 1 steps (100%) done
Complete log: .snakemake/log/2024-11-14T130659.002459.snakemake.log
unlocking
removing lock
removing lock
removed all locks
